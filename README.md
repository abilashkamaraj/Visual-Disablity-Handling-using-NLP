# Visual-Disablity-Handling-using-NLP
Image captioning model for visually challenged people

Normal people look at an image and give the response to the answer for any questions utilizing their insight. But visually impaired people have to rely on others for help to interpret and understand an image. Several technologies have been developed for assistance of visually impaired people. One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image captioning systems. Being able to describe the content of an image using properly formed English sentences is a challenging task, but it could have great impact by helping visually impaired people better understand their surroundings. These descriptions can then be read out loud to the visually impaired. This project proposes to present a memory-efficient Image captioning model which is trained with batches of data to avoid memory error during training of the model.

The Image captioning model uses a Convolutional Neural Network to extract features from an image. These features are then fed into a Long Short Term Memory network to generate a description of the image in a valid English sentence. The description is converted to audio output using a python Text-to-Speech Application Programming Interface. The proposed model can generate highly descriptive captions that can potentially aid the visually impaired people.
